# -*- coding: utf-8 -*-
"""INTERNSHIP (Sentiment Analysis Using LSTM).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vvJ2Ya5Ja1oaCPJHdxfWYpSF6sbTzjs
"""

# Importing required libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load IMDB dataset top 10,000 most frequent wordS
num_words = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)

# Pad sequences to ensure uniform input length
max_len = 200
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# Build LSTM Model
model = Sequential()
model.add(Embedding(input_dim=num_words, output_dim=128, input_length=max_len))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.2)

# Evaluate model on test data
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype("int32")

# Accuracy and Confusion Matrix
acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {acc:.2f}")

cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Neg','Pos'], yticklabels=['Neg','Pos'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Predict sample sentences
word_index = imdb.get_word_index()
reverse_index = {value: key for key, value in word_index.items()}

def encode_text(text):
    tokens = text.lower().split()
    encoded = [word_index.get(word, 2) for word in tokens]  # 2 is for unknown words
    return pad_sequences([encoded], maxlen=max_len)

def predict_sentiment(text):
    encoded = encode_text(text)
    pred = model.predict(encoded)[0][0]
    sentiment = "Positive" if pred > 0.5 else "Negative"
    print(f"Input: \"{text}\"\nPredicted Sentiment: {sentiment}")

predict_sentiment("I love this movie!")

predict_sentiment("I hate this movie!")

